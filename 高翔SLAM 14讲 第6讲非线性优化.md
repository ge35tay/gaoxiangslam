# 高翔SLAM 14讲 第6讲非线性优化

现在已知SLAM视觉情况下的表达，SLAM模型的运动方程和观测方程中的位姿可以用变换矩阵来描述，然后用李代数来优化。观测方程由相机成像模型给出，其中内参是随相机固定的，而外参是相机的位姿。

本章主要讨论在有噪声的情况下如何进行准确的状态估计， 在SLAM中同一个点会被相机在不同时刻内多次观测，同一个相机在每个时刻观测到的点也不止一个，这些因素交织在一起使得我们有了更多的约束。



## 1. 状态估计问题

### 1.1 最大后验与最大似然

SLAM由一个状态方程和运动方程构成
$$
\left\{
\begin{aligned}
x_k & = & f(x_{k-1}, u_k) + w_k \\
z_{k, j} & = & h(y_j, x_k) + v_{k,j} \\
\end{aligned}
\right.
$$
在运动方程中， $x_k$ 是相机的位姿，通常用变换矩阵$T_k$，旋转矩阵或者李代数$exp(\xi_k^{\land})$表示;

在观测方程中， 假设机器人在 $x_k$ 点对特征点 $y_j$ 观测，并对应到图像上像素点 $z_{k, j}$ . 
$$
sz_{k,j} = K exp(\xi^{\land})y_j
$$
 $K$ 为相机内参， $s$ 是像素点的距离， $z_{k,j}$ 与 $y_j$ 都必须以齐次坐标描述。

对于噪声项 $w_k$ 与 $v_{k,j}$ 我们假设均满足零均值的高斯分布
$$
w_k \sim N(0, R_k),  v_k \sim N(0, Q_{k,j})
$$
我们希望通过这些带噪声的数据 $z$ 与 $u$ 来推断位姿 $x$ 与地图 $y$ 以及他们的概率分布，主要有两种方式：

> 1.   **卡尔曼滤波**，  只关心当前时刻的状态估计 $ x_k$ ， 而对之前的状态不多考虑。
> 2.  **非线性优化**， 使用所有采集到的数据进行状态估计



从概率学角度看， 在非线性优化中，我们把所有待估计的变量放在一个状态变量中:
$$
x = \left\{ x_1, ..., x_N, y_1, ..., y_N \right\}
$$
现在对机器人状态的估计就是求已知输入数据 $u$ 和观测数据 $z$ 的条件下，计算 $x$ 的概率分布 $P(x|z. u)$

特别的，当没有测量运动的传感器，只有相机图像输入时，相当于估计 $P(x|z)$ 的条件概率分布， 忽略图像在时间上的关系，把它们看成一对没有关系的图片，该问题即为 **Structure from motion** 问题。利用贝叶斯法则有：
$$
P(x|z) = \frac {P(z|x)P(x)}{P(z)} \propto P(z|x)P(x)
$$
其中，$P(x|z)$ 为 **后验概率** (在收到某个消息之后，接收端所了解到的该消息发送的概率)， $P(x|z)$ 为 **似然概率** (z确定，x不确定)，$P(x)$ 为**先验概率**， 求解后验分布是困难的，但是求一个状态最优估计，使得在状况下后验概率最大化是可行的, 因为贝叶斯公式里分母是与待估计状态无关，可以忽略
$$
x^{*}_{MAP} = argmax P(x|z) = argmaxP(z|x)P(x)
$$
但是往往先验概率也是不知道的，因为机器人往往不知道自己到了哪，所以整个问题就变成了 **最大似然估计 （Maximize Likelihood Estimation MLE）**, 即在什么养的状态下，最可能产生观测到的数据
$$
x^{*}_{MLE} = argmax P(z|x)
$$
 

### 1.2 最小二乘法

对于观测模型，我们有：
$$
z_{k,j} = h(y_j, x_k) + v_{k,j}
$$
噪声符合高斯分布 $v_k \sim N(0, Q_{k,j})$ ，所以观测数据的条件概率为:
$$
P(z_{j,k}|x_k, y_j) = N(h(y_j, x_k), Q_{k,j})
$$
为了求这个最大似然估计，我们常常用 **最小化负对数** 的方法来求一个高斯分布的最大似然。

考虑一个任意的高维高斯分布 $x \sim N(\mu ,\Sigma)$ , 他的概率密度函数展开形式为:
$$
P(x) = \frac{1}{\sqrt{(2\pi)^{N}det(\Sigma)}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
取负对数可得:
$$
-ln(P(x)) = \frac{1}{2}ln((2\pi)^Ndet(\Sigma)) + \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
$$
右边第一项与状态无关(噪声的方差)，所以只需要最小化右侧的二次项，就可求得最大似然估计。

代入上式可得:
$$
x^{*} = argmin((z_{k,j} - h(x_k, y_j))^TQ^{-1}_{k,j}(z_{k,j} - h(x_k, y_j)))
$$
该式等价于最小化噪声项的平方。

我们定义数据与估计值的误差为:
$$
e_{v,k} = x_k - f(x_{k-1}, u_k) \\
e_{y,j,k} = z_{k,j} - h(x_k, y_j)
$$
并求该误差的平方值和:
$$
J = \sum_{k}e_{v,k}^TR_k^{-1}e_{v,k} + \sum_{k}\sum_{j}e_{y,k,j}^TQ_{k,j}^{-1}e_{y,k,j}
$$
这样我们就得到了一个兼顾运动方程与观测方程的最小二乘问题

- 首先，整个问题的目标函数由许多个误差的加权的平方和组成，每个误差项简单，可计算，例如运动只与 $x_{k-1}$ , $x_k$ 有关，观测误差只与 $x_k, y_j$ 有关。
- 整体误差有许多小型误差项之和组成的问题，其增量方程的求解具有一定的稀疏性，使得他们在大规模时亦可求解
- 如果用李代数表示，则该问题时无约束的最小二乘问题



## 2. 非线性最小二乘

对于一个二乘问题，考虑到他形式可能的复杂性，因此我们可以用迭代的方式，不直接求解导数，而是从一个初始值出发，不断更新当前的优化变量，使得目标函数下降。

> 1. 给定某个初始值 $x_0$
> 2. 对于第 $k$ 次迭代，寻找一个增量 $\Delta x_k$, 使得 $||f(x_k + \Delta x_k)||^2_2$ 达到极小值
> 3. 若 $\Delta x_k$ 足够小则停止
> 4. 否则 令 $x_{k+1} = x_k + \Delta x_k$， 返回2



如何确定 增量$\Delta x_k$



### 2.1 一阶和二阶梯度法

求解增量的最直观方式是将目标函数在 $x$ 附近泰勒展开:
$$
||f(x+\Delta x)||^2_2 \approx ||f(x)||_2^2 + J(x)\Delta x + \frac{1}{2}\Delta x^TH\Delta x
$$
这里 $J$ 是 $||f(x)||^2$ 关于 $x$ 的导数（雅可比矩阵），而 $H$ 是二阶导数海塞矩阵。

- 如果我们只保留一阶梯度，则增量方向为：
  $$
  \Delta x^{*} = -J^T(x)
  $$
  直观意义就是沿着反向梯度方向前进即可，当然也需要在该方向上去一个步长 $\lambda$ ,求得其最快的下降方式，这种方法称为最速下降法

- 若保留二阶梯度信息，则增量方程为:
  $$
  \Delta x^{*} = argmin ||f(x)||_2^2 + J(x)\Delta x + \frac{1}{2}\Delta x^TH\Delta x
  $$
  对右侧关于 $\Delta x$  求导并使其为0，则可得：
  $$
  H\Delta x = -J^T
  $$



但以上这两种方法都存在问题，最速下降法过于贪心，容易走出锯齿路线，增加迭代次数，而牛顿法需要计算目标函数的 $H$ 海塞矩阵。



### 2.1.1 Gauss-Newton

高斯牛顿方法损失将** $f(x)$  (而不是目标函数 $f(x)^2$ ) ** 进行一阶的泰勒展开:
$$
f(x + \Delta x) \approx f(x) + J(x)\Delta x
$$
这里的雅可比矩阵 $J(x)$ 是$f(x)$ 对 $x$ 的导数。我们的目标是寻找下降矢量 $\Delta x$ ， 使得 $||f(x+\Delta x)||^2$ 达到最小，即:
$$
\Delta x^{*} = argmin \frac{1}{2}||f(x) + J(x)\Delta x||^2
$$
展开得:
$$
\frac{1}{2} ||f(x) + J(x)\Delta x||^2 = \frac{1}{2}(f(x + J(x)\Delta x)^T(f(x + J(x)\Delta x)\\
= \frac{1}{2}(||f(x)||_2^2 + 2f(x)^TJ(x)\Delta x + \Delta x^T J(x)^TJ(x)\Delta x)
$$
上式对 $\Delta x$ 求导,并使其为0可得：
$$
2J(x)^T f(x) + 2J(x)^TJ(x)\Delta x= 0 \\
J(x)^TJ(x)\Delta x = -J(x)^Tf(x)
$$
z这是一个线性方程组，称其为高斯牛顿方程，我们定义左边的系数为 $H$, 即把 $J(x)^TJ(x)$ 看作是海塞矩阵的近似，省略了对 $H$的计算，右边定义为 $g$, 则上式变为:
$$
H\Delta x= g
$$
所以Gauss Newton的算法步骤可以总结为:

>1. 给定初始值 $x_0$
>2. 对于第k次迭代， 求出当前f(x)的雅可比矩阵 $J(x_k)$ 和误差 $f(x_k)$
>3. 求解增量方程: $H \Delta x_k = g$
>4. 若 $\Delta x_k$ 足够小，则停止，否则令 $x_{k+1} = x_k + \Delta x_k$ ,返回2



但是在实际问题中，$H$ 是正定矩阵而 $J^TJ$ 是半正定的，这可能导致 $J^TJ$ 具有奇异性且病态。而且如果求出来的步长太大，也会导致局部近似 $ f(x + \Delta x) \approx f(x) + J(x)\Delta x$ 不够准确



### 2.1.2 Levenberg-Marquadt

由于Gauss-Newton方法中近似二阶泰勒展开只能在展开点附近有较好的近似效果，因此我们可以在 $\Delta x $ 上添加一个信赖区域 . 为了确定信赖区域的范围大小，我们计算:
$$
\rho = \frac{f(x+\Delta x) - f(x)}{J(x)\Delta x}
$$
分子为实际下降的值而分母为近似下降的值，若

- $\rho$ 很小，则说明实际减小的值远少于近似减小的值，应该减小近似范围
- $\rho$ 接近于1， 则近似是好的
- $\rho$ 比较大，则说明实际下降的比预计的更大，可以放大近似范围

> 1. 给定初始值 $x_0$ ， 以及初始优化半径 $\mu$
>
> 2. 对于第 k 次迭代，求解:
>    $$
>    min_{\Delta x_k}\frac{1}{2}||f(x_k) + J(x_k)\Delta x_k||^2,\\
>    s.t. ||D \Delta x_k||^2 \leq \mu
>    $$
>    $\mu$ 为信赖区域半径， 我们把增量限定在一个半径为$\mu$ 的球中，认为只有在这个球内是有效的，带上$D$ 可以把球看成是一个椭球
>
> 3. 计算 $\rho$
>
> 4. 若 $\rho$ > $\frac{3}{4}$ ， 则 $\mu$ = 2 $\mu$
>
> 5. 若 $\rho$ < $\frac{1}{4}$ ， 则 $\mu$ = 0.5 $\mu$
>
> 6. 若 $\rho$ 大于某阈值，认为近似可行，令 $x_{k+1} = x_k + \Delta x_k$
>
> 7. 判断是否收敛，若不收敛则返回2



对于2这个优化问题，我们用  $Lagrange$ 乘子转化为一个无约束问题:
$$
min_{\Delta x_k}\frac{1}{2}||f(x_k) + J(x_k)\Delta x_k||^2 +
\frac{\lambda}{2}||D \Delta x_k||^2
$$
将其展开后我们可以得到
$$
(H + \lambda D^TD)\Delta x = g
$$
我们可以看到，若参数 $\lambda$ 比较小， H占主导地位，说明二次次近似拟合得好
